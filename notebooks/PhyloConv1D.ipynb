{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is required in order to be able to do relative imports like phcnn.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import (Lambda, MaxPooling1D, Flatten,\n",
    "                          Dropout, Dense, Input)\n",
    "from keras.models import Model\n",
    "from keras.backend import floatx\n",
    "\n",
    "from phcnn.layers import PhyloConv1D, euclidean_distances\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "\n",
    "Parameters from convolutional layer. nb_neighbors is the number of neighbors to be convoluted together, nb_filters is the number of convolutional filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_neighbors = 4\n",
    "nb_filters = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of data\n",
    "\n",
    "We need to expand Xs to be of the shape (filters, nb_samples, nb_features), so we apply a np.expand_dims to signal that we have only one filter. Futhermore we need to have y in a categorical form so we apply to_categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xs = pd.read_csv('../datasets/ibd_dataset/HS_CDf/Sokol_16S_taxa_HS_CDf_commsamp_training.txt',\n",
    "                sep='\\t', header=0, index_col=0).as_matrix()\n",
    "nb_features = Xs.shape[1]\n",
    "Xs = np.expand_dims(Xs, axis=-1)\n",
    "y = np.loadtxt('../datasets/ibd_dataset/HS_CDf/Sokol_16S_taxa_HS_CDf_commsamp_training_lab.txt', dtype=np.int)\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Futhermore we need to import the MDS coordinates for our features. Unfortunately, keras has a problem. It requires a batch size and it requires all the batch sizes for the inputs to be the same. So we need to add padding to simulate that we have a sample size even for the coordinates. We choosed to do it in the most straigthforward way possibile, we simply duplicate the coordinate matrix for every sample. We will drop such padding after the matrix is loaded in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = pd.read_csv('../datasets/coordinates/coordinates_cdf.txt',\n",
    "                sep='\\t', header=0, index_col=0).as_matrix()\n",
    "nb_coordinates = c.shape[0]\n",
    "coord = np.empty((Xs.shape[0],) + c.shape, dtype=np.float64)\n",
    "for i in range(Xs.shape[0]):\n",
    "    coord[i] = c\n",
    "coord = np.expand_dims(coord, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = Input(shape=(nb_features, 1), name=\"data\", dtype=floatx())\n",
    "coordinates = Input(shape=(nb_coordinates, nb_features, 1),\n",
    "                            name=\"coordinates\", dtype=floatx())\n",
    "\n",
    "conv_layer = data\n",
    "# We remove the padding that we added to work around keras limitations\n",
    "conv_crd = Lambda(lambda c: c[0], output_shape=lambda s: (s[1:]))(coordinates)\n",
    "\n",
    "distances = euclidean_distances(conv_crd)\n",
    "conv_layer, conv_crd = PhyloConv1D(distances, nb_neighbors,\n",
    "                                   nb_filters, activation='relu')([conv_layer, conv_crd])\n",
    "\n",
    "max = MaxPooling1D(pool_size=2, padding=\"valid\")(conv_layer)\n",
    "flatt = Flatten()(max)\n",
    "drop = Dropout(0.25)(Dense(units=64, activation='relu')(flatt))\n",
    "output = Dense(units=2, kernel_initializer=\"he_normal\",\n",
    "               activation=\"softmax\", name='output')(drop)\n",
    "\n",
    "model = Model(inputs=[data, coordinates], outputs=output)\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "69/69 [==============================] - 0s - loss: 0.6956     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f183b5c2ac8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[Xs, coord], y=y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
