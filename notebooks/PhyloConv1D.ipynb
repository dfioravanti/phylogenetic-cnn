{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is required in order to be able to do relative imports like phcnn.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Change PYTHONPATH to allow for relative import\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import (Lambda, MaxPooling1D, Flatten,\n",
    "                          Dropout, Dense, Input)\n",
    "from keras.models import Model\n",
    "from keras.backend import floatx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5110 on context None\n",
      "Mapped name None to device cuda0: Tesla K80 (0BFE:00:00.0)\n"
     ]
    }
   ],
   "source": [
    "from phcnn.layers import PhyloConv1D, euclidean_distances\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "\n",
    "Parameters from convolutional layer. nb_neighbors is the number of neighbors to be convoluted together, nb_filters is the number of convolutional filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_neighbors = 4\n",
    "nb_filters = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of data\n",
    "\n",
    "We need to expand Xs to be of the shape (filters, nb_samples, nb_features), so we apply a np.expand_dims to signal that we have only one filter. Futhermore we need to have y in a categorical form so we apply to_categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xs = pd.read_csv('../datasets/ibd_dataset/HS_CDf/Sokol_16S_taxa_HS_CDf_commsamp_training.txt',\n",
    "                 sep='\\t', header=0, index_col=0).as_matrix()\n",
    "nb_features = Xs.shape[1]\n",
    "Xs = np.expand_dims(Xs, axis=-1)\n",
    "y = np.loadtxt('../datasets/ibd_dataset/HS_CDf/Sokol_16S_taxa_HS_CDf_commsamp_training_lab.txt', dtype=np.int)\n",
    "Y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Futhermore we need to import the MDS coordinates for our features. \n",
    "\n",
    "Pre-computed coordinates have been made available in the repository in the `datasets/coordinates`, for each of the diseases included in the **IBD** dataset.\n",
    "\n",
    "Keras uses fast symbolic mathematical libraries as a backend, such as TensorFlow and Theano.\n",
    "\n",
    "A downside of using these libraries is that the shape and size of your data must be defined once up front and held constant regardless of whether you are training your network or making predictions.\n",
    "\n",
    "To fit the coordinate matrix as a valid Keras Tensor, we need to pre-process the numpy array to properly match dimensions. In more details, the first dimension of a tensor object must correspond to `n_samples`, that is the number of samples (_per batch_).\n",
    "\n",
    "Thus we need to replicate **feature** coordinates for all the samples so that each sample  provide  padding to loaded numpy array for the coordinates. This is because, i We choosed to do it in the most straigthforward way possibile, we simply duplicate the coordinate matrix for every sample. We will drop such padding after the matrix is loaded in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = pd.read_csv('../datasets/coordinates/coordinates_cdf.txt',\n",
    "                sep='\\t', header=0, index_col=0).as_matrix()\n",
    "nb_coordinates = C.shape[0]\n",
    "Coords = np.empty((Xs.shape[0],) + C.shape, dtype=np.float64)\n",
    "for i in range(Xs.shape[0]):\n",
    "    Coords[i] = C\n",
    "# add last dimension, i.e. channel, necessary for the Convolution\n",
    "# operator to work.\n",
    "Coords = np.expand_dims(Coords, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = Input(shape=(nb_features, 1), name=\"data\", dtype=floatx())\n",
    "coordinates = Input(shape=(nb_coordinates, nb_features, 1),\n",
    "                            name=\"coordinates\", dtype=floatx())\n",
    "\n",
    "conv_layer = data\n",
    "# We remove the padding that we added to work around keras limitations\n",
    "conv_crd = Lambda(lambda c: c[0], output_shape=lambda s: (s[1:]))(coordinates)\n",
    "\n",
    "distances = euclidean_distances(conv_crd)\n",
    "conv_layer, conv_crd = PhyloConv1D(distances, nb_neighbors,\n",
    "                                   nb_filters, activation='relu')([conv_layer, conv_crd])\n",
    "\n",
    "max = MaxPooling1D(pool_size=2, padding=\"valid\")(conv_layer)\n",
    "flatt = Flatten()(max)\n",
    "drop = Dropout(0.25)(Dense(units=64, activation='relu')(flatt))\n",
    "output = Dense(units=2, kernel_initializer=\"he_normal\",\n",
    "               activation=\"softmax\", name='output')(drop)\n",
    "\n",
    "model = Model(inputs=[data, coordinates], outputs=output)\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "69/69 [==============================] - 13s - loss: 0.6921     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f61b02d3978>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[Xs, Coords], y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
